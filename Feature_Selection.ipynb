{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qinsiqi/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RandomizedLogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_feat_selection(X_train, y_train, X_valid, y_valid, random_state):\n",
    "    \"\"\"\n",
    "    Feature selection based on the scores given to the features by an \n",
    "    XGB Classifier.\n",
    "    \"\"\"\n",
    "    #Parameters of the xgb classifier to be used for feature selection\n",
    "    params = {'eta': 0.09,\n",
    "              'max_depth': 6,\n",
    "              'subsample': 0.5,\n",
    "              'colsample_bytree': 0.5,\n",
    "              'objective': 'multi:softprob',\n",
    "              'eval_metric': 'mlogloss',\n",
    "              'num_class': 12}\n",
    "    num_rounds = 1000\n",
    "    xg_train = xgboost.DMatrix(X_train, label=y_train)  \n",
    "    xg_valid = xgboost.DMatrix(X_valid, label=y_valid)  \n",
    "    watchlist = [(xg_train,'train'), (xg_valid, 'validation')]\n",
    "    #Training the model and stopping at the best iteration\n",
    "    xgb = xgboost.train(params, xg_train, num_rounds, watchlist,\n",
    "                        early_stopping_rounds=10)\n",
    "    #Getting the scores for each feature\n",
    "    f_score = xgb.get_fscore()\n",
    "    feats = np.zeros(X_train.shape[1])\n",
    "    #Scores are given in the format => fn:x meaning n-th feature has a value x.\n",
    "    for k,v in f_score.items():\n",
    "        feats[int(k[1:])] = v\n",
    "    #Normalizing the scores to [0,1.]\n",
    "    feats = feats/float(np.max(feats))\n",
    "    \n",
    "    np.save('save/feat_sel_xgb.npy', feats)\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_reg_feat_selection(X_train, y_train, X_valid, y_valid, random_state):\n",
    "    \"\"\"\n",
    "    Feature selection based on the scores given to the features by the \n",
    "    RandomizedLogisticRegression algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    rlr = RandomizedLogisticRegression(C=[0.001, 0.01, 0.1, 1.], \n",
    "                                       sample_fraction=0.7,\n",
    "                                       n_resampling=200, selection_threshold=0.25,\n",
    "                                       verbose=5, n_jobs=-1, random_state=0)                                   \n",
    "    rlr.fit(X_train, y_train)\n",
    "    np.save('save/feat_sel_log_reg.npy', rlr.scores_)\n",
    "    \n",
    "    return rlr.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_forest_selection(X_train, y_train, X_valid, y_valid, random_state):\n",
    "    \"\"\"\n",
    "    Feature selection based on the scores given to the features by the \n",
    "    RandomForest algorithm.\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(n_estimators=400, n_jobs=-1, \n",
    "                                criterion='gini',\n",
    "                                max_depth=15,\n",
    "                                max_features=0.2,\n",
    "                                max_leaf_nodes = 20,\n",
    "                                min_samples_split=50,\n",
    "                                random_state=random_state, verbose=10)\n",
    "    rf.fit(X_train, y_train) \n",
    "    feat_imp = rf.feature_importances_\n",
    "    np.save('save/feat_sel_random_forest.npy', feat_imp)\n",
    "    \n",
    "#    th = np.sort(feat_imp)[::-1][int(len(feat_imp)*7./10.)]\n",
    "#    feats = feat_imp > th    \n",
    "    \n",
    "    return feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_feat_sel(X_train, X_valid, X_test, f_type='xgboost', th=0.001):\n",
    "    \"\"\"\n",
    "    Selects features with highest scores according to a precomputed feature\n",
    "    scoring.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    X_train: training set\n",
    "    X_valid: validation set\n",
    "    X_test: test set\n",
    "    f_type: string\n",
    "          Type of feature scoring system to be used. Possible values are:\n",
    "          xgboost, for feature selection based on xgboost scoring.\n",
    "          log_reg, for feature selection based on logistic regression scoring.\n",
    "    th: float\n",
    "        Threshold. Features with a score higher than th are kept the others are\n",
    "        discarded.\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    X_train, X_valid, X_test: Train, validation and test sets after feature extraction.\n",
    "    \"\"\"\n",
    "    if f_type == 'xgboost':\n",
    "        scores = np.load('save/feat_sel_xgb.npy')\n",
    "    elif f_type == 'log_reg':\n",
    "        scores = np.load('save/feat_sel_log_reg.npy')\n",
    "    elif f_type == 'random_forest':\n",
    "        scores = np.load('save/feat_sel_random_forest.npy')\n",
    "        \n",
    "    feats = scores > th\n",
    "    \n",
    "    X_train = X_train[:, feats]\n",
    "    X_valid = X_valid[:, feats]\n",
    "    X_test = X_test[:, feats]\n",
    "    \n",
    "    print 'Keeping %s features from the original %s' %(X_train.shape[1], feats.shape[0])\n",
    "    \n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_feat_sel_by_percent(X_train, X_valid, X_test, f_type='xgboost', percent=0.7):\n",
    "    \"\"\"\n",
    "    Selects features with highest scores according to a precomputed feature\n",
    "    scoring. The top percent features are kept.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    X_train: training set\n",
    "    X_valid: validation set\n",
    "    X_test: test set\n",
    "    f_type: string\n",
    "          Type of feature scoring system to be used. Possible values are:\n",
    "          xgboost, for feature selection based on xgboost scoring.\n",
    "          log_reg, for feature selection based on logistic regression scoring.\n",
    "    percent: float [0, 1]\n",
    "        percent*100 gives the percent of features to keep.\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    X_train, X_valid, X_test: Train, validation and test sets after feature extraction.\n",
    "    \"\"\"       \n",
    "    if f_type == 'xgboost':\n",
    "        scores = np.load('save/feat_sel_xgb.npy')\n",
    "    elif f_type == 'log_reg':\n",
    "        scores = np.load('save/feat_sel_log_reg.npy')\n",
    "    elif f_type == 'random_forest':\n",
    "        scores = np.load('save/feat_sel_random_forest.npy')\n",
    "        \n",
    "    th = np.sort(scores)[::-1][int(len(scores)*7./10.)]   \n",
    "    \n",
    "    feats = scores > th\n",
    "    \n",
    "    X_train = X_train[:, feats]\n",
    "    X_valid = X_valid[:, feats]\n",
    "    X_test = X_test[:, feats]\n",
    "    \n",
    "    print 'Keeping %s features from the original %s' %(X_train.shape[1], feats.shape[0])\n",
    "    \n",
    "    return X_train, X_valid, X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
